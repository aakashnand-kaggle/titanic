---
title: "Titanic Random Forest"
author: "Aakash Nand"
date: "16/07/2019"
output: 
  html_document :
    toc: true
    theme: united
---
## 0. Disclaimer
I have referred the Megan Risdal's Kaggle Titanic Notebook. This is my first attempt to understand the kaggle kernel written by her. All credits to `Megan Risdal`
Link : https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic

## 1. Introduction
This is my first kernel for Titanic competition. I will add more details here later. Lets get started.

### 1.1 Loading and Reading the Data
```{r setup, echo=TRUE,message=FALSE}
library(ggplot2)
library(dplyr)
library(mice)
library(randomForest)
library(scales)
library(ggthemes)
```

### 1.2 Reading the data
```{r}
train<-read.csv("./titanic_data/train.csv",stringsAsFactors = FALSE)
test<-read.csv("./titanic_data/test.csv",stringsAsFactors = FALSE)
full<-bind_rows(train,test)
str(full)
```

Lets understand each variable :

| Variable    | Description                                                         |
|-------------|---------------------------------------------------------------------|
| PassengerId | Unique Passenger ID                                                 |
| Survived    | binary flag 0=Dead,1=Survived                                       |
| Pclass      | A proxy for socio-economic status (SES),1=Upper, 2=Middle, 3=Lower  |
| Name        | Passenger Name                                                      |
| Sex         | Gender of Passenger                                                 |
| Age         | Passenger Age                                                       |
| SibSp       | Number of siblings/spouses aboard                                   |
| Parch       | Number of parents/children aboard                                   |
| Ticket      | Ticket number                                                       |
| Fare        | Passenger fare                                                      |
| Cabin       | Cabin number                                                        |
| Embarked    | Port of Embarkation, C = Cherbourg, Q = Queenstown, S = Southampton |


## 2. Feature Engineering

### 2.1 Lets grab titles
```{r}
full$title<-gsub('(.*, )|(\\..*)', '', full$Name)

table(full$Sex,full$title)
```

Lets combine less known title to `rare_titles` variable
```{r}
rare_titles<-c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don', 
                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
full$title[full$title=='Mlle']<-'Miss'
full$title[full$title=='Ms']<-'Miss'
full$title[full$title=='Mme']<-'Mrs'
full$title[full$title %in% rare_titles]<-'rare_titles'
table(full$Sex,full$title)
```
Lets extract surnames from name
```{r}
full$surname<-sapply(full$Name,  function(x) strsplit(x, split = '[,.]')[[1]][1])
```
Now that we are done splitting names into some useful columns, lets try to understand family size from data.

#### 2.2 Analysis by family size
```{r}
full$fsize<-full$Parch+full$SibSp+1
# Create a family variable 
full$Family <- paste(full$surname, full$fsize, sep='_')
```
Lets visualize family sizes by survival rate
```{r}
ggplot(full[1:891,],aes(x=fsize,fill=factor(Survived))) + geom_bar(stat = 'count',position = 'dodge') + scale_x_continuous(breaks = c(1:11)) + labs(x='Family Size') + theme_few()
```


Lets create three groups for various family sizes
```{r}
full$fsizeD[full$fsize==1]<-'singles'
full$fsizeD[full$fsize<=4 & full$fsize>1]<-'small'
full$fsizeD[full$fsize>4]<-'large'
mosaicplot(table(full$fsizeD,full$Survived),main = 'Survival by family size',shade = TRUE)
```

#### 2.3 Cabin Information
```{r}
full$Cabin[1:20]
print("Lets extract the first letter from Cabin ")
full$deck<-factor(sapply(full$Cabin, function(x) strsplit(x, NULL)[[1]][1]))
```

## 3. Handling Missing Values

Missing values are present in this dataset in various columns such as `embarkment`, `price` and `age`
Lets handle them one by one
### 3.1 Emabrkment
```{r}
full[full$Embarked=="",'PassengerId']
```
We can see that there 2 passengers (62, 830) whose embarkment is missing. To handle these values, we will first remove them and see if we can guess those values from remianing dataset. We will check if they share any common statistics. The most relevant variables which could share some statistics are `fare` and `class`
```{r}
embark<-full %>% filter(PassengerId!=62 & PassengerId!=830)

# Lets plot bar-chart using ggplot2
ggplot(embark,aes(x=Embarked, y=Fare, fill=factor(Pclass))) +
  geom_boxplot() +
  geom_hline(aes(yintercept=80),color='red',linetype='dashed',lwd=1) +
  scale_y_continuous(labels = dollar_format()) +
  theme_few()
```

Great we can see that Median of Class C matches with fare paid by passenger 62 and 830. So we can safely replace `NA` values of embarkment of passenger 62 and 830 by `C` that is `Charbourg`

```{r}
full$Embarked[c(62,830)]<-'C'
```


### 3.2 Fare
```{r}
knitr::kable(full[is.na(full$Fare),],format = 'markdown')
```

We can see that `Mr.Thomas` seems to forgot to register the Fare for his trip. From his record we can see that Mr.Thomas embarked from Southampton ('S')

```{r}
ggplot(full[full$Embarked=='S' & full$Pclass=='3',],aes(x=Fare)) +
  geom_density(fill = '#99d6ff', alpha=0.4) +
  geom_vline(aes(xintercept=median(Fare, na.rm=T)),
    colour='red', linetype='dashed', lwd=0.8) +
  scale_x_continuous(labels=dollar_format(),breaks = seq(0,60,4)) +
  theme_few()
```
From this visualization, it seems quite reasonable to replace the NA Fare value with median for their class and embarkment which is $8.00
```{r}
full$Fare[1044]<-median(full[full$Pclass == '3' & full$Embarked == 'S', ]$Fare, na.rm = TRUE)
```

### 3.4 Age and deck
```{r}
knitr::kable(sapply(full,function(x)sum(is.na(x))),format = 'markdown')
```
From above we can see that we still have to deal with `NA` from Age and deck column. To deal with these many NA values we will use Predictive Imputation technique. We will use mice library and Random Forest Imputation technique. 

```{r}
factor_vars <- c('PassengerId','Pclass','Sex','Embarked',
                 'title','surname','Family','fsizeD')
full[factor_vars] <- lapply(full[factor_vars], function(x) as.factor(x))

set.seed(129)

mice_mod <- mice(full[, !names(full) %in% c('PassengerId','Name','Ticket','Cabin','Family','Surname','Survived')], method='rf') 
```
save the mice output
```{r}
mice_output<-complete(mice_mod)
```

Lets confirm if `mice` has affected age distribution across the data by histogram.

```{r}
par(mfrow=c(1,2))
hist(full$Age, freq=F, main='Age: Original Data', 
  col='darkgreen', ylim=c(0,0.04))
hist(mice_output$Age, freq=F, main='Age: MICE Output', 
  col='lightgreen', ylim=c(0,0.04))
```

Looks good. So lets replace the original age column by mice_data

```{r}
full$Age<-mice_output$Age
sum(is.na(full$Age))
```

### 3.5 Child and Mother features
Now that we know everyone’s age, we can create a couple of new age-dependent variables: Child and Mother. A child will simply be someone under 18 years of age and a mother is a passenger who is 1) female, 2) is over 18, 3) has more than 0 children (no kidding!), and 4) does not have the title ‘Miss’.

```{r}
# First we'll look at the relationship between age & survival
ggplot(full[1:891,], aes(Age, fill = factor(Survived))) + 
  geom_histogram() + 
  # I include Sex since we know (a priori) it's a significant predictor
  facet_grid(.~Sex) + 
  theme_few()
```

```{r}
# Create the column child, and indicate whether child or adult
full$Child[full$Age < 18] <- 'Child'
full$Child[full$Age >= 18] <- 'Adult'

# Show counts
table(full$Child, full$Survived)
```
Looks like being a child doesn’t hurt, but it’s not going to necessarily save you either! We will finish off our feature engineering by creating the Mother variable. Maybe we can hope that mothers are more likely to have survived on the Titanic.

```{r}
# Adding Mother variable
full$Mother <- 'Not Mother'
full$Mother[full$Sex == 'female' & full$Parch > 0 & full$Age > 18 & full$title != 'Miss'] <- 'Mother'
# Show counts
table(full$Mother, full$Survived)

# Finish by factorizing our two new factor variables
full$Child  <- factor(full$Child)
full$Mother <- factor(full$Mother)
```
```{r,message=FALSE}
md.pattern(full)
```

## 4. Prediction
At last we’re ready to predict who survives among passengers of the Titanic based on variables that we carefully curated and treated for missing values. For this, we will rely on the randomForest classification algorithm; we spent all that time on imputation, after all.

### 4.1 Training set and Test set
```{r}
# Split the data back into a train set and a test set
train <- full[1:891,]
test <- full[892:1309,]
```
### 4.2 Building model
```{r}
# Set a random seed
set.seed(754)

# Build the model (note: not all possible variables are used)
rf_model <- randomForest(factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + 
                                            Fare + Embarked + title + 
                                            fsizeD + Child + Mother,
                                            data = train)

# Show model error
plot(rf_model, ylim=c(0,0.36))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
```

### 4.3 Prediction
```{r}
# Predict using the test set
prediction <- predict(rf_model, test)

# Save the solution to a dataframe with two columns: PassengerId and Survived (prediction)
solution <- data.frame(PassengerID = test$PassengerId, Survived = prediction)

# Write the solution to file
write.csv(solution, file = 'rf_mod_Solution.csv', row.names = F)
```
















